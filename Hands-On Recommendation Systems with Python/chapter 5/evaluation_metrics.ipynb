{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "### Accuracy\n",
    "Accuracy is the most widely used metric to gauge the performance of a classification\n",
    "model. It is the ratio of the number of correct predictions to the total number of predictions\n",
    "made by the model\n",
    "\n",
    "### Root mean square error\n",
    "The Root Mean Square Error (or RMSE) is a metric widely used to gauge the performance\n",
    "of regressors.\n",
    "\n",
    "\n",
    "### Binary Classification metrics\n",
    "\n",
    "- **True positive (TP)**: True positive refers to all cases where the actual and the\n",
    "predicted classes are both positive\n",
    "- **True negative (TN)**: True negative refers to all cases where the actual and the\n",
    "predicted classes are both negative\n",
    "- **False positive (FP)**: These are all the cases where the actual class is negative but\n",
    "the predicted class is positive\n",
    "- **False negative (FN)**: These are all the cases where the actual class is positive but\n",
    "the predicted class is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "The precision is the ratio of the number of positive cases that were correct to all the cases\n",
    "that were identified as positive.\n",
    "\n",
    "### Recall\n",
    "The recall is the ratio of the number of positive cases that were identified to the all positive\n",
    "cases present in the dataset\n",
    "\n",
    "### F1 Score\n",
    "The F1 score is a metric that conveys the balance between precision and recall. It is the\n",
    "harmonic mean of the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
