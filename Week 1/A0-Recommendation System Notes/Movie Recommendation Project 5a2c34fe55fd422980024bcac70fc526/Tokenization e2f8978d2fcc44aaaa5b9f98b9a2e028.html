<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Tokenization</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-interactiveBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="e2f8978d-2fcc-44aa-aa5b-9f98b9a2e028" class="page sans"><header><h1 class="page-title">Tokenization</h1><p class="page-description"></p></header><div class="page-body"><p id="3dd2afa3-9ba3-4182-a3fc-50e08a2400f1" class="">
</p><ul id="ee92120a-1ec9-471b-823a-cee3b72db7e8" class="bulleted-list"><li style="list-style-type:disc">Tokens are building blocks of natural language</li></ul><ul id="4b0173ac-524e-462c-bbe4-9823c66f5305" class="bulleted-list"><li style="list-style-type:disc">Tokenization is a way of separating a piece of text into smaller units called token. Token can be of 3 types:<ul id="3320676a-9f80-4ef1-b5b4-d39471b1a6bf" class="bulleted-list"><li style="list-style-type:circle">Word</li></ul><ul id="c0828e6f-b69b-41ac-a8b1-b1fc3f91610b" class="bulleted-list"><li style="list-style-type:circle">Character</li></ul><ul id="5241ff6c-023a-4302-8219-3615de907ea2" class="bulleted-list"><li style="list-style-type:circle">Subword (n-gram characters)</li></ul></li></ul><p id="1d340a9a-192f-4c6f-9813-42ee293c3462" class="">
</p><h3 id="5ec0a610-799d-40d9-bcbb-8b135d7f00d5" class="">Example</h3><p id="bde85635-953c-417a-94a2-f06893bf3134" class=""><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>“Never give up”</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p><ul id="bd5b5685-7525-445c-8a49-816944512e00" class="bulleted-list"><li style="list-style-type:disc"><strong><strong><strong>Word Token: </strong></strong></strong>Never, give, up</li></ul><p id="1a003f22-e8b3-4013-b63b-256e16b7bbaf" class=""><strong><strong><strong>“Smarter”</strong></strong></strong></p><ul id="2e199617-8c0f-4bb7-a7bb-05191a4663ce" class="bulleted-list"><li style="list-style-type:disc"><strong><strong><strong>Character Token: </strong></strong></strong>S-m-a-r-t-e-r</li></ul><ul id="252aba14-1e7b-455d-8cb4-2b9f85304a3d" class="bulleted-list"><li style="list-style-type:disc"><strong><strong><strong>Subword Token: </strong></strong></strong>smart-er</li></ul><p id="2ce9f2fb-3f5f-4f52-9251-2369a6320a4d" class="">
</p><h3 id="cc1e9afe-d025-48fc-94df-19ea29c52b8e" class="">Need for tokenization</h3><figure id="b349062f-bb89-4b44-aa20-b1f70b8639b4" class="image"><a href="Tokenization%20e2f8978d2fcc44aaaa5b9f98b9a2e028/Untitled.png"><img style="width:788px" src="Tokenization%20e2f8978d2fcc44aaaa5b9f98b9a2e028/Untitled.png"/></a></figure><ul id="3dbe38de-c5b4-455b-9a12-b2137e95a5b9" class="bulleted-list"><li style="list-style-type:disc">State of the art Deep learning architectures process raw text at token level</li></ul><ul id="56d2c576-cf97-4a81-9bf2-534814ad9f0f" class="bulleted-list"><li style="list-style-type:disc">RNN receives each token at a particular timestep</li></ul><ul id="7bc2951e-4c18-4a4f-9993-e169fbb08b70" class="bulleted-list"><li style="list-style-type:disc">Traditional NLP approaches such as Count Vectorizer and TF-IDF use vocabulary as features. Each word in the vocabulary is treated as a unique feature.</li></ul><p id="c74d9709-0ba8-4b9f-b36f-7297416162d2" class="">
</p><p id="db2c322d-63b1-4070-a6a3-1499ddd2c510" class=""><em><em><em><em><em><em><em><em><em><em><em><em><em>Creating Vocabulary is the ultimate goal of Tokenization.</em></em></em></em></em></em></em></em></em></em></em></em></em></p><p id="abaf9a2d-6596-417e-a53e-1bbfe3dcb37b" class="">
</p><p id="f1b5e21d-bb4b-4315-aa9a-4c42778b7f1d" class="">
</p><h3 id="06645e5b-2ca8-41c8-a166-052bb6336776" class="">Word Tokenization</h3><ul id="727f13d0-a8c2-4daf-96e2-014b7c8ac432" class="bulleted-list"><li style="list-style-type:disc">split into words</li></ul><ul id="2c9f8c82-d2a7-4bcc-9f03-a0253dada665" class="bulleted-list"><li style="list-style-type:disc">depending upon delimiters, different word-level tokens are formed<ul id="352707a4-84f7-4bd6-8713-dcbe738908af" class="bulleted-list"><li style="list-style-type:circle">Eg: Word2Vec, GloVe</li></ul></li></ul><p id="097e8998-4064-42db-bd51-901e115c2047" class="">
</p><p id="a9e01151-4956-42e3-ae31-dd2fc1eeb37d" class=""><strong><strong><strong><strong>Drawbacks</strong></strong></strong></strong><div class="indented"><ul id="b409a44f-e6ab-483f-82cf-7411afb7a79c" class="bulleted-list"><li style="list-style-type:disc">dealing with <strong><strong><strong><strong><strong><strong><strong><strong><strong>Out of Vocabulary (OOV) </strong></strong></strong></strong></strong></strong></strong></strong></strong>words</li></ul><ul id="18f390b4-dde4-48b2-879a-ce7e9749a787" class="bulleted-list"><li style="list-style-type:disc">OOV are new words which are encountered during testing and do not exist in the vocabulary</li></ul><ul id="7d96e9a6-c197-4598-a6b5-781dcad1eaa5" class="bulleted-list"><li style="list-style-type:disc"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Solution for OOV:</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong><ul id="ccfb3c22-65df-49bf-8074-11da11c7d228" class="bulleted-list"><li style="list-style-type:circle">form vocabulary with top K frequent words and replace rare words in training data with unknown tokens (UNK). helps model to learn the representation of OOV words in terms of UNK tokens.</li></ul><ul id="0bd0c205-5a75-4f98-ac14-d9c0b06df74b" class="bulleted-list"><li style="list-style-type:circle">during test, any word not in vocabulary will be mapped to a UNK token.</li></ul><ul id="58b1e59b-4c0c-40c7-beab-98f5fbab40ce" class="bulleted-list"><li style="list-style-type:circle"><strong><strong><strong><strong><strong><strong><strong><strong><strong>Problem: </strong></strong></strong></strong></strong></strong></strong></strong></strong>entire information of the word is lost as we are mapping OOV to UNK. structure might be useful though. all OOV word get the same representation</li></ul></li></ul><ul id="6b0ab90e-f19f-4ac4-9df2-abfa440d29da" class="bulleted-list"><li style="list-style-type:disc">word token is connected to size of vocabulary. Pre-trained models are trained on large volume of text corpus. Vocabulary explodes with such large corpus.</li></ul></div></p><h3 id="df0d5162-aa81-4321-a843-c46de2824cf6" class="">Character Tokenization</h3><ul id="5aba7f38-b026-4e45-82be-d138601c56c1" class="bulleted-list"><li style="list-style-type:disc">splits into chracters</li></ul><ul id="f6420419-aa29-4ae9-b7f6-e7f43b9244c0" class="bulleted-list"><li style="list-style-type:disc">overcomes drawbacks of word tokenization</li></ul><ul id="b7c0d88e-ee7f-4df6-b705-8258e42de0db" class="bulleted-list"><li style="list-style-type:disc">handles OOV words coherently by preserving information of the word. breaks down the OOV word into characters and represents the word in term of these characters</li></ul><ul id="10e4dd78-74cd-41d9-ab6c-6706a9f16d4b" class="bulleted-list"><li style="list-style-type:disc">limits the size of vocabulary → 26</li></ul><p id="6fc9cfdc-517d-4508-bf0b-7ecadee2c575" class="">
</p><p id="0a94245f-3b59-4703-aec8-3112dc8202e6" class=""><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Drawbacks:</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong><div class="indented"><ul id="6c2d865f-0cbb-47a3-b27c-6b0f6a63bb7e" class="bulleted-list"><li style="list-style-type:disc">length of input and output increases, making it difficult to learn relation between characters to form meaningful words</li></ul></div></p><p id="9bf3f004-8980-4c3a-a4e0-7058efe6dd40" class="">
</p><h3 id="a86982c8-1c5e-4b0f-8fd4-6f19bdc8910a" class="">Subword Tokenization</h3><ul id="e2dd4814-c134-4b84-9989-1a6e9645d49c" class="bulleted-list"><li style="list-style-type:disc">splits into subwords (n-gram characters)</li></ul><ul id="d054235e-7b0f-4e2b-a985-834d0dc4074b" class="bulleted-list"><li style="list-style-type:disc">Example: lower → low-er</li></ul><ul id="ccb04815-15dc-4b1d-828e-4751737b6343" class="bulleted-list"><li style="list-style-type:disc">Transformer based model (SOTA) rely on subword tokenization.</li></ul><ul id="7d616721-d40e-41f7-ba07-d9cf1081bce9" class="bulleted-list"><li style="list-style-type:disc">Algorithm: Byte Pair Encoding (BPE)</li></ul><p id="566995f1-98f4-4c72-b81b-c9de0546a442" class="">
</p><p id="c23a1a4f-394d-469b-a8f8-881c6547a6b9" class="">
</p><p id="d0de503f-a030-4405-83c4-5bf01b11fdff" class="">
</p><p id="1ee5ebd9-9baa-4868-8d55-0af47e20f9f8" class="">
</p><p id="5c231200-0b54-456e-9801-705afd7f69a7" class=""><strong><strong><strong> </strong></strong></strong></p></div></article></body></html>